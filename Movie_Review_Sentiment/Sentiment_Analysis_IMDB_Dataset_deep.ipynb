{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_Analysis_IMDB_Dataset_deep.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFwpEv7E1myT",
        "outputId": "b683ea27-ef47-4fa5-ecca-ecc0428bf499"
      },
      "source": [
        "%cd drive/MyDrive/Github/Natural-Language-Processing/Sentiment\\ Analysis/IMDB movie review sentiment/"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Github/Natural-Language-Processing/Sentiment Analysis/IMDB movie review sentiment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVHVmYEm9Orm"
      },
      "source": [
        "## Importing necessary modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WE7o2uRctKqN",
        "outputId": "4c2435a1-7a9d-4f9c-d98e-dd7c3aea27bb"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "import re"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jW8x8X619TFC"
      },
      "source": [
        "## Loading dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "id": "hcHnuMgFzLaY",
        "outputId": "3c93e4f4-c300-4a06-9de1-545e5ee7229c"
      },
      "source": [
        "train = pd.read_csv(\"Train.csv\")\n",
        "valid = pd.read_csv(\"Valid.csv\")\n",
        "test = pd.read_csv(\"Test.csv\")\n",
        "\n",
        "print('train size:', len(train))\n",
        "print('valid size:', len(valid))\n",
        "print('test size:', len(test))\n",
        "train.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train size: 40000\n",
            "valid size: 5000\n",
            "test size: 5000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I grew up (b. 1965) watching and loving the Th...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>When I put this movie in my DVD player, and sa...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Why do people who do not know what a particula...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Even though I have great interest in Biblical ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Im a die hard Dads Army fan and nothing will e...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  label\n",
              "0  I grew up (b. 1965) watching and loving the Th...      0\n",
              "1  When I put this movie in my DVD player, and sa...      0\n",
              "2  Why do people who do not know what a particula...      0\n",
              "3  Even though I have great interest in Biblical ...      0\n",
              "4  Im a die hard Dads Army fan and nothing will e...      1"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJLxSDDH9gyS"
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9LMikVn2rSq"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "unwanted_symbols = re.compile('[/{}\\[\\]\\|@,;]')\n",
        "def review_tokenizer(review):\n",
        "    review = review.lower()\n",
        "    review = unwanted_symbols.sub(' ', review) # replace unwanted_symbols by space in text\n",
        "    tokenized_review = word_tokenize(review)\n",
        "    tokenized_review = [stemmer.stem(word) for word in tokenized_review]\n",
        "    return tokenized_review"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfAuDiR8_VRf"
      },
      "source": [
        "## Tokenizing dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOovnueC2wYp"
      },
      "source": [
        "X_train = []\n",
        "X_valid = []\n",
        "X_test = []\n",
        "\n",
        "for review in train.text:\n",
        "    tokenized_review = review_tokenizer(review.strip())\n",
        "    X_train.append(tokenized_review)\n",
        "y_train = list(train.label)\n",
        "\n",
        "for review in valid.text:\n",
        "    tokenized_review = review_tokenizer(review.strip())\n",
        "    X_valid.append(tokenized_review)\n",
        "y_valid = list(valid.label)\n",
        "\n",
        "for review in test.text:\n",
        "    tokenized_review = review_tokenizer(review.strip())\n",
        "    X_test.append(tokenized_review)\n",
        "y_test = list(test.label)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6unh2vHN_sCV"
      },
      "source": [
        "## Creating vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuJFj5Jy_d7-",
        "outputId": "61fe0e48-a42d-48fa-abae-15954e7140a6"
      },
      "source": [
        "vocab = {'<PAD': 0, '</e>': 1, '<UNK>': 2} \n",
        "\n",
        "for review in X_train: \n",
        "    for word in review:\n",
        "        if word not in vocab: \n",
        "            vocab[word] = len(vocab)\n",
        "    \n",
        "print(\"vocab size\",len(vocab))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size 115487\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dq6A8-X2_zSw"
      },
      "source": [
        "## Converting tokenized data to tensors of indices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zl2n_FlD4Fum"
      },
      "source": [
        "def review_to_indices(review, vocab, unk_token='<UNK>'):\n",
        "    review_indices = []\n",
        "    unk_id = vocab[unk_token]\n",
        "    for word in review:\n",
        "        word_id = vocab[word] if word in vocab else unk_id\n",
        "        review_indices.append(word_id) \n",
        "    return review_indices\n",
        "\n",
        "def data_to_tensor(data):\n",
        "    data_indices = []\n",
        "    for review in data:\n",
        "      review_indices = review_to_indices(review, vocab, unk_token='<UNK>')\n",
        "     \n",
        "      data_indices.append(review_indices)\n",
        "    return data_indices\n"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ebilic3ctc7Q"
      },
      "source": [
        "X_train_indices = data_to_tensor(X_train)\n",
        "X_valid_indices = data_to_tensor(X_valid)\n",
        "X_test_indices = data_to_tensor(X_test)\n"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CW6MCBXmCCzv"
      },
      "source": [
        "# Converting targets to one hot vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAfdRtm3Wcnw"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "y_train_hot = to_categorical(y_train, num_classes=2) #class 0: negative and class 1: possitive\n",
        "y_valid_hot = to_categorical(y_valid, num_classes=2)\n",
        "y_test_hot = to_categorical(y_test, num_classes=2)"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NayHeHItNi_"
      },
      "source": [
        "## Bucketing dataset by length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EMHBrM3ryyK"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "#sentence length finder\n",
        "def element_length_fn(x, y):\n",
        "    return tf.shape(x)[0]\n",
        "\n",
        "#creating data generator\n",
        "def data_generator(inputs, targets):\n",
        "  dataset = tf.data.Dataset.from_generator(\n",
        "      generator = lambda: ((x, y) for x, y in zip(inputs, targets)),\n",
        "      output_shapes=([None], [2,]),\n",
        "      output_types=(tf.int32, tf.int32)\n",
        "  )\n",
        "\n",
        "  dataset = dataset.bucket_by_sequence_length(\n",
        "          element_length_func = element_length_fn,\n",
        "          bucket_batch_sizes = [512, 512, 256, 128, 128, 64, 32, 16],\n",
        "          bucket_boundaries = [32, 64, 128, 256, 512, 1024, 2048],   #sentences lengths\n",
        "          padding_values=(0, 0))\n",
        "  return dataset\n"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZPCZOASvGyo"
      },
      "source": [
        "train_generator = data_generator(X_train_indices, y_train_hot)\n",
        "valid_generator = data_generator(X_valid_indices, y_valid_hot)\n",
        "test_generator = data_generator(X_test_indices, y_test_hot)"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHsYnJAKAOQK"
      },
      "source": [
        "## Defining model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RediZlux-bJ"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Embedding, Lambda\n",
        "from tensorflow.keras import backend as K"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPSuSrlctL7B",
        "outputId": "0e0807df-74be-47e7-a576-2e063708bebe"
      },
      "source": [
        "vocab_size = len(vocab)+1\n",
        "embedding_dim = 64\n",
        "\n",
        "def mean(x):\n",
        "  return K.mean(x, axis=1)\n",
        "  \n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Lambda(mean))\n",
        "model.add(Dense(2, activation='softmax')) #two classes. one for positive and another for negative sentiment\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_23\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_23 (Embedding)     (None, None, 64)          7391232   \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, None, 32)          2080      \n",
            "_________________________________________________________________\n",
            "lambda_7 (Lambda)            (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             (None, 2)                 66        \n",
            "=================================================================\n",
            "Total params: 7,393,378\n",
            "Trainable params: 7,393,378\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWSvMYi-CW6m"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5kqwIoVc7mU",
        "outputId": "b60820f7-75fc-454a-f941-fa96ec37f8a8"
      },
      "source": [
        "model.fit(train_generator, epochs=5, validation_data=(valid_generator))"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "328/328 [==============================] - 50s 151ms/step - loss: 0.5326 - accuracy: 0.7463 - val_loss: 0.3525 - val_accuracy: 0.8574\n",
            "Epoch 2/5\n",
            "328/328 [==============================] - 49s 149ms/step - loss: 0.2807 - accuracy: 0.8923 - val_loss: 0.2892 - val_accuracy: 0.8830\n",
            "Epoch 3/5\n",
            "328/328 [==============================] - 49s 150ms/step - loss: 0.2158 - accuracy: 0.9205 - val_loss: 0.2689 - val_accuracy: 0.8940\n",
            "Epoch 4/5\n",
            "328/328 [==============================] - 49s 150ms/step - loss: 0.1771 - accuracy: 0.9374 - val_loss: 0.2621 - val_accuracy: 0.8966\n",
            "Epoch 5/5\n",
            "328/328 [==============================] - 49s 150ms/step - loss: 0.1481 - accuracy: 0.9505 - val_loss: 0.2633 - val_accuracy: 0.8992\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f97cf8d9f90>"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brO1oaTVvTvu"
      },
      "source": [
        "## Evaluating model on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5U2ntFYbbhP_",
        "outputId": "e6930f44-6fd0-477a-936a-f77257990814"
      },
      "source": [
        "loss, accuracy = model.evaluate(test_generator)\n",
        "print(\"test loss:\", loss, \"\\n test accuracy:\", accuracy)"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "46/46 [==============================] - 2s 44ms/step - loss: 0.2692 - accuracy: 0.9042\n",
            "test loss: 0.2691759467124939 \n",
            "test accuracy: 0.90420001745224\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRYE9H__vYIK"
      },
      "source": [
        "## Predicting custom review sentiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQJqzB3Ls-D3"
      },
      "source": [
        "def predict_review(review, model):\n",
        "  tokenized_review = review_tokenizer(review)\n",
        "  review_indices = review_to_indices(tokenized_review, vocab)\n",
        "  review_indices = np.array(review_indices)\n",
        "  review_indices = np.expand_dims(review_indices, axis=0)\n",
        "  pred = model.predict(review_indices)\n",
        "  is_positive =  pred[0][1] > pred[0][0]\n",
        "  if is_positive:\n",
        "    print(\"Positive review\")\n",
        "  else:\n",
        "    print(\"Negative review\")"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NFuLhQbcZjV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ea02117-9daa-4916-d617-2d9bdd329166"
      },
      "source": [
        "review = \"Wow! That's my reaction to pretty much the entire film that writer-director Christopher Nolan has made called Inception. Since this is the first movie I've seen of his that isn't part of the Batman series (correction, I did see The Prestige and review it back in 2006), I just marvel at how creatively compelling his work truly is when he literally uses his imagination to the fullest of his ability. To make a more apt comparison: What Lost is to television, Inception is to the movies-the ability to truly take you to places you've never been before. Plus, what a great cast: Leonardo DiCaprio, Joseph Gordon-Levitt, Ellen Page, Tom Hardy, Ken Watanabe, Dileep Rao, Cillian Murphy, Tom Berenger, Marion Cotillard, Pete Postlethwaite, and Michael Caine among others. All deserve the kudos they're getting and more. No wonder it's been No. 1 at the box office these past three weeks! This movie isn't very easy to describe so I won't even try. I'll just highly recommend this to anyone who wants their minds challenged to the fullest extent of their ability and just leave it at that.\"\n",
        "predict_review(review, model)"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive review\n"
          ]
        }
      ]
    }
  ]
}